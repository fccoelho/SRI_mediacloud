{
 "metadata": {
  "name": "",
  "signature": "sha256:f1ffb3964f14693a3fa3ab4bd55e257dd9d32e761a48f506081e452e73715117"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
      "from sklearn import metrics\n",
      "from operator import itemgetter\n",
      "from sklearn.metrics import classification_report\n",
      "import numpy as np\n",
      "import os\n",
      "import pymongo\n",
      "import nltk\n",
      "from collections import Counter\n",
      "import random"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING: pylab import has clobbered these variables: ['random']\n",
        "`%matplotlib` prevents importing * from pylab and numpy\n"
       ]
      }
     ],
     "prompt_number": 84
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "client = pymongo.MongoClient()\n",
      "mongo = client.UOL_CRAWLER.urls\n",
      "db = mongo.find({'parsed':{'$exists':True}}, {'cleaned_text':1, 'name':1, '_id':0})\n",
      "labels = []\n",
      "data = []\n",
      "db = list(db)\n",
      "random.shuffle(db)\n",
      "for d in db:\n",
      "    if len(d['cleaned_text']) < 100:\n",
      "        continue\n",
      "    if d['name'] == 'eleicoes':\n",
      "        continue\n",
      "    labels.append(d['name'])\n",
      "    data.append(d['cleaned_text'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 89
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lengths = [len(d) for d in data]\n",
      "Counter(labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 90,
       "text": [
        "Counter({u'cinema': 799, u'carros': 754, u'ciencia': 702, u'musica': 606, u'cotidiano': 576, u'politica': 540, u'saude': 354, u'tecnologia': 320, u'internacional': 312, u'esporte': 240, u'economia': 211})"
       ]
      }
     ],
     "prompt_number": 90
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "hist(lengths, bins=50);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD/CAYAAAAT87ocAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSZIkSZIkSZIkSZIkSZJOJ/8Psl54utjS8eEAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7f7517201ad0>"
       ]
      }
     ],
     "prompt_number": 91
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "trainset_size = int(round(len(data)*0.75)) # i chose this threshold arbitrarily...to discuss\n",
      "print 'The training set size for this classifier is ' + str(trainset_size) + '\\n'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The training set size for this classifier is 4061\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 92
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train = np.array([''.join(el) for el in data[:trainset_size]])\n",
      "y_train = np.array([el for el in labels[:trainset_size]])\n",
      "\n",
      "X_test = np.array([''.join(el) for el in data[trainset_size+1:len(data)]]) \n",
      "y_test = np.array([el for el in labels[trainset_size+1:len(labels)]]) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 93
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = CountVectorizer(min_df=2, \n",
      " ngram_range=(1, 1), \n",
      " stop_words=nltk.corpus.stopwords.words('portuguese'), \n",
      " strip_accents='unicode')\n",
      "\n",
      "test_string = unicode(data[0])\n",
      "\n",
      "#print \"Example string: \" + test_string\n",
      "#print \"Preprocessed string: \" + vectorizer.build_preprocessor()(test_string)\n",
      "#print \"Tokenized string:\" + str(vectorizer.build_tokenizer()(test_string))\n",
      "#print \"N-gram data string:\" + str(vectorizer.build_analyzer()(test_string))\n",
      "#print \"\\n\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 94
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train = vectorizer.fit_transform(X_train)\n",
      "X_test = vectorizer.transform(X_test)\n",
      "\n",
      "nb_classifier = MultinomialNB().fit(X_train, y_train)\n",
      "\n",
      "y_nb_predicted = nb_classifier.predict(X_test)\n",
      "\n",
      "print \"MODEL: Multinomial Naive Bayes\\n\"\n",
      "\n",
      "print 'The precision for this classifier is ' + str(metrics.precision_score(y_test, y_nb_predicted))\n",
      "print 'The recall for this classifier is ' + str(metrics.recall_score(y_test, y_nb_predicted))\n",
      "print 'The f1 for this classifier is ' + str(metrics.f1_score(y_test, y_nb_predicted))\n",
      "print 'The accuracy for this classifier is ' + str(metrics.accuracy_score(y_test, y_nb_predicted))\n",
      "\n",
      "print '\\nHere is the classification report:'\n",
      "print classification_report(y_test, y_nb_predicted)\n",
      "\n",
      "#simple thing to do would be to up the n-grams to bigrams; try varying ngram_range from (1, 1) to (1, 2)\n",
      "#we could also modify the vectorizer to stem or lemmatize\n",
      "print '\\nHere is the confusion matrix:'\n",
      "print metrics.confusion_matrix(y_test, y_nb_predicted, labels=np.unique(labels))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MODEL: Multinomial Naive Bayes\n",
        "\n",
        "The precision for this classifier is 0.867891290515\n",
        "The recall for this classifier is 0.865384615385\n",
        "The f1 for this classifier is 0.864276095227\n",
        "The accuracy for this classifier is 0.865384615385\n",
        "\n",
        "Here is the classification report:\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "     carros       1.00      0.96      0.98       180\n",
        "    ciencia       0.78      0.87      0.82       167\n",
        "     cinema       0.89      0.98      0.93       214\n",
        "  cotidiano       0.72      0.77      0.74       144\n",
        "   economia       0.83      0.57      0.67        53\n",
        "    esporte       0.96      0.93      0.95        56\n",
        "internacional       0.88      0.75      0.81        79\n",
        "     musica       0.92      0.91      0.91       143\n",
        "   politica       0.88      0.87      0.88       147\n",
        "      saude       0.80      0.83      0.82        94\n",
        " tecnologia       0.86      0.73      0.79        75\n",
        "\n",
        "avg / total       0.87      0.87      0.86      1352\n",
        "\n",
        "\n",
        "Here is the confusion matrix:\n",
        "[[173   1   0   2   1   0   0   2   1   0   0]\n",
        " [  0 145   2   7   1   1   2   0   0   7   2]\n",
        " [  0   2 209   0   0   0   1   1   0   0   1]\n",
        " [  0   7   4 111   2   1   0   4   7   6   2]\n",
        " [  0   1   1  12  30   0   4   0   1   2   2]\n",
        " [  0   0   2   0   0  52   0   1   0   0   1]\n",
        " [  0   4   1   2   1   0  59   1   7   3   1]\n",
        " [  0   1  10   2   0   0   0 130   0   0   0]\n",
        " [  0   4   2  11   1   0   0   1 128   0   0]\n",
        " [  0  10   1   2   0   0   1   2   0  78   0]\n",
        " [  0  10   3   5   0   0   0   0   1   1  55]]\n"
       ]
      }
     ],
     "prompt_number": 95
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import LinearSVC\n",
      "\n",
      "svm_classifier = LinearSVC().fit(X_train, y_train)\n",
      "\n",
      "y_svm_predicted = svm_classifier.predict(X_test)\n",
      "print \"MODEL: Linear SVC\\n\"\n",
      "\n",
      "print 'The precision for this classifier is ' + str(metrics.precision_score(y_test, y_svm_predicted))\n",
      "print 'The recall for this classifier is ' + str(metrics.recall_score(y_test, y_svm_predicted))\n",
      "print 'The f1 for this classifier is ' + str(metrics.f1_score(y_test, y_svm_predicted))\n",
      "print 'The accuracy for this classifier is ' + str(metrics.accuracy_score(y_test, y_svm_predicted))\n",
      "\n",
      "print '\\nHere is the classification report:'\n",
      "print classification_report(y_test, y_svm_predicted)\n",
      "\n",
      "#simple thing to do would be to up the n-grams to bigrams; try varying ngram_range from (1, 1) to (1, 2)\n",
      "#we could also modify the vectorizer to stem or lemmatize\n",
      "print '\\nHere is the confusion matrix:'\n",
      "print metrics.confusion_matrix(y_test, y_svm_predicted, labels=np.unique(labels))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MODEL: Linear SVC\n",
        "\n",
        "The precision for this classifier is 0.843961172269\n",
        "The recall for this classifier is 0.840976331361\n",
        "The f1 for this classifier is 0.841348427217\n",
        "The accuracy for this classifier is 0.840976331361\n",
        "\n",
        "Here is the classification report:\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "     carros       0.98      0.96      0.97       180\n",
        "    ciencia       0.80      0.84      0.82       167\n",
        "     cinema       0.93      0.97      0.95       214\n",
        "  cotidiano       0.61      0.69      0.65       144\n",
        "   economia       0.77      0.62      0.69        53\n",
        "    esporte       1.00      0.91      0.95        56\n",
        "internacional       0.76      0.72      0.74        79\n",
        "     musica       0.96      0.92      0.94       143\n",
        "   politica       0.79      0.84      0.82       147\n",
        "      saude       0.76      0.69      0.73        94\n",
        " tecnologia       0.84      0.76      0.80        75\n",
        "\n",
        "avg / total       0.84      0.84      0.84      1352\n",
        "\n",
        "\n",
        "Here is the confusion matrix:\n",
        "[[172   1   0   3   1   0   0   0   3   0   0]\n",
        " [  1 140   0  10   0   0   6   1   2   3   4]\n",
        " [  0   1 207   1   1   0   3   1   0   0   0]\n",
        " [  1   9   1  99   2   0   2   2  17   9   2]\n",
        " [  2   0   1   8  33   0   3   0   3   0   3]\n",
        " [  0   1   1   0   0  51   1   1   0   0   1]\n",
        " [  0   6   1   5   3   0  57   0   4   3   0]\n",
        " [  0   0   8   2   1   0   0 132   0   0   0]\n",
        " [  0   3   1  16   0   0   1   0 124   2   0]\n",
        " [  0  10   1  11   1   0   2   1   2  65   1]\n",
        " [  0   5   2   6   1   0   0   0   1   3  57]]\n"
       ]
      }
     ],
     "prompt_number": 96
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "maxent_classifier = LogisticRegression().fit(X_train, y_train)\n",
      "\n",
      "y_maxent_predicted = maxent_classifier.predict(X_test)\n",
      "print \"MODEL: Maximum Entropy\\n\"\n",
      "\n",
      "print 'The precision for this classifier is ' + str(metrics.precision_score(y_test, y_maxent_predicted))\n",
      "print 'The recall for this classifier is ' + str(metrics.recall_score(y_test, y_maxent_predicted))\n",
      "print 'The f1 for this classifier is ' + str(metrics.f1_score(y_test, y_maxent_predicted))\n",
      "print 'The accuracy for this classifier is ' + str(metrics.accuracy_score(y_test, y_maxent_predicted))\n",
      "\n",
      "print '\\nHere is the classification report:'\n",
      "print classification_report(y_test, y_maxent_predicted)\n",
      "\n",
      "#simple thing to do would be to up the n-grams to bigrams; try varying ngram_range from (1, 1) to (1, 2)\n",
      "#we could also modify the vectorizer to stem or lemmatize\n",
      "print '\\nHere is the confusion matrix:'\n",
      "print metrics.confusion_matrix(y_test, y_maxent_predicted, labels=np.unique(labels))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MODEL: Maximum Entropy\n",
        "\n",
        "The precision for this classifier is 0.793283794678\n",
        "The recall for this classifier is 0.513617184503\n",
        "The f1 for this classifier is 0.566736649495\n",
        "The accuracy for this classifier is 0.513617184503\n",
        "\n",
        "Here is the classification report:\n",
        "             precision    recall  f1-score   support\n",
        "\n",
        "     carros       0.97      0.69      0.80       252\n",
        "    ciencia       0.84      0.62      0.71       250\n",
        "     cinema       0.94      0.78      0.85       254\n",
        "  cotidiano       0.76      0.48      0.59       274\n",
        "   economia       0.16      0.88      0.27       228\n",
        "    esporte       0.97      0.25      0.40       241\n",
        "internacional       0.84      0.33      0.47       200\n",
        "     musica       0.93      0.56      0.69       227\n",
        "   politica       0.76      0.50      0.60       199\n",
        "      saude       0.75      0.27      0.40       232\n",
        " tecnologia       0.79      0.26      0.39       250\n",
        "\n",
        "avg / total       0.79      0.51      0.57      2607\n",
        "\n",
        "\n",
        "Here is the confusion matrix:\n",
        "[[173   0   0   0  75   0   1   0   1   0   2]\n",
        " [  1 154   0  11  61   0   5   2   3   8   5]\n",
        " [  0   0 199   1  49   0   1   3   0   1   0]\n",
        " [  0   8   0 131 110   1   1   2  12   7   2]\n",
        " [  1   2   1  10 201   0   0   0   7   0   6]\n",
        " [  0   0   0   4 175  61   0   0   0   1   0]\n",
        " [  2   3   3   3 114   0  66   0   6   2   1]\n",
        " [  0   2   5   0  93   1   0 126   0   0   0]\n",
        " [  0   0   0   8  86   0   1   1 100   2   1]\n",
        " [  0  12   1   3 146   0   3   1   3  63   0]\n",
        " [  2   3   3   2 173   0   1   1   0   0  65]]\n"
       ]
      }
     ],
     "prompt_number": 82
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim import corpora, models, similarities\n",
      "from itertools import chain\n",
      "import nltk\n",
      "from nltk.corpus import stopwords\n",
      "from operator import itemgetter\n",
      "import re\n",
      "\n",
      "url_pattern = r'https?:\\/\\/(.*[\\r\\n]*)+'\n",
      "\n",
      "documents = data\n",
      "stoplist =  nltk.corpus.stopwords.words('portuguese')\n",
      "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
      "    for document in documents]\n",
      "\n",
      "dictionary = corpora.Dictionary(texts)\n",
      "corpus = [dictionary.doc2bow(text) for text in texts]\n",
      "\n",
      "tfidf = models.TfidfModel(corpus) \n",
      "corpus_tfidf = tfidf[corpus]\n",
      "\n",
      "#lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=100)\n",
      "#lsi.print_topics(20)\n",
      "\n",
      "n_topics = 20\n",
      "lda = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=n_topics)\n",
      "\n",
      "for i in range(0, n_topics):\n",
      "    temp = lda.show_topic(i, 10)\n",
      "    terms = []\n",
      "    for term in temp:\n",
      "        terms.append(term[1])\n",
      "    print \"Top 10 terms for topic #\" + str(i) + \": \"+ \", \".join(terms)\n",
      " \n",
      "print \n",
      "print 'Which LDA topic maximally describes a document?\\n'\n",
      "print 'Original document: ' + documents[1]\n",
      "print 'Preprocessed document: ' + str(texts[1])\n",
      "print 'Matrix Market format: ' + str(corpus[1])\n",
      "print 'Topic probability mixture: ' + str(lda[corpus[1]])\n",
      "print 'Maximally probable topic: topic #' + str(max(lda[corpus[1]],key=itemgetter(1))[0])\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ImportError",
       "evalue": "No module named gensim",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-83-006ca435e5b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mchain\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0moperator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mitemgetter\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mImportError\u001b[0m: No module named gensim"
       ]
      }
     ],
     "prompt_number": 83
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}